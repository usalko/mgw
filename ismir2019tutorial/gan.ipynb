{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salu133445/ismir2019tutorial/blob/main/gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgXVFl5SmZvC"
      },
      "source": [
        "# Generating MNIST Digit with GANs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mArAbLs2mBhd"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3wBMsOGwyky"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CJ_cVBuhk13r"
      },
      "outputs": [],
      "source": [
        "#!pip install -q torch torchvision matplotlib tqdm livelossplot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIsiOQ5mw5vw"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zOkT9h38krfZ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from ipywidgets import interact, IntSlider\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "from livelossplot import PlotLosses\n",
        "from livelossplot.outputs import MatplotlibPlot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VyXuXFtoLxL"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PA14sQ-YoTvW"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "feature_dim = 28 * 28\n",
        "\n",
        "# Training\n",
        "batch_size = 64\n",
        "latent_dim = 128\n",
        "n_steps = 20000\n",
        "\n",
        "# Sampling\n",
        "sample_interval = 100\n",
        "n_cols = 15\n",
        "n_rows = 2\n",
        "n_samples = 30\n",
        "assert n_samples == n_cols * n_rows, (\n",
        "    \"Number of samples and number of images per column/row do not match.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6fauic_H2wt"
      },
      "source": [
        "## Training data loader function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "716vG6H-ldA7"
      },
      "outputs": [],
      "source": [
        "def get_data_loader():\n",
        "    \"\"\"Download the MNIST dataset and create a data loader.\"\"\"\n",
        "    # MNIST training set\n",
        "    mnist_train = torchvision.datasets.MNIST(\n",
        "        root=\"./dataset\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=torchvision.transforms.ToTensor()\n",
        "    )\n",
        "    # MNIST test set\n",
        "    mnist_test = torchvision.datasets.MNIST(\n",
        "        root=\"./dataset\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=torchvision.transforms.ToTensor()\n",
        "    )\n",
        "    # Use both traning and test sets\n",
        "    dataset = torch.utils.data.ConcatDataset([mnist_train, mnist_test])\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, drop_last=True, shuffle=True\n",
        "    )\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh9yTxj5IiHK"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2D-XmLjSr_3p"
      },
      "outputs": [],
      "source": [
        "def images_to_vectors(images):\n",
        "    \"\"\"Convert images to vectors.\"\"\"\n",
        "    return images.view(images.size(0), 784)\n",
        "\n",
        "def vectors_to_images(vectors):\n",
        "    \"\"\"Convert vectors to images.\"\"\"\n",
        "    return vectors.view(vectors.size(0), 1, 28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rrn1nmIQlG"
      },
      "source": [
        "## Neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZWPAxfkmsIWn"
      },
      "outputs": [],
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    \"\"\"A multilayer perceptron (MLP) based generator. The generator takes as\n",
        "    input a latent vector and outputs a fake sample.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden0 = torch.nn.Linear(latent_dim, 128)\n",
        "        self.hidden1 = torch.nn.Linear(128, 256)\n",
        "        self.hidden2 = torch.nn.Linear(256, 512)\n",
        "        self.out = torch.nn.Linear(512, feature_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.hidden0(x))\n",
        "        x = torch.nn.functional.relu(self.hidden1(x))\n",
        "        x = torch.nn.functional.relu(self.hidden2(x))\n",
        "        x = self.out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kczm8A8Nl78i"
      },
      "outputs": [],
      "source": [
        "class Discriminator(torch.nn.Module):\n",
        "    \"\"\"A multilayer perceptron (MLP) based discriminator. The discriminator\n",
        "    takes as input either a real sample (in the training data) or a fake sample\n",
        "    (generated by the generator) and outputs a scalar indicating its authentity.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden0 = torch.nn.Linear(feature_dim, 512)\n",
        "        self.dropout0 = torch.nn.Dropout()\n",
        "        self.hidden1 = torch.nn.Linear(512, 256)\n",
        "        self.dropout1 = torch.nn.Dropout()\n",
        "        self.hidden2 = torch.nn.Linear(256, 128)\n",
        "        self.dropout2 = torch.nn.Dropout()\n",
        "        self.out = torch.nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout0(torch.nn.functional.leaky_relu(self.hidden0(x)))\n",
        "        x = self.dropout1(torch.nn.functional.leaky_relu(self.hidden1(x)))\n",
        "        x = self.dropout2(torch.nn.functional.leaky_relu(self.hidden2(x)))\n",
        "        x = self.out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiPl8DYCI7pC"
      },
      "source": [
        "## Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5wngyfaaObas"
      },
      "outputs": [],
      "source": [
        "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
        "    \"\"\"Compute the gradient penalty for regularization. Intuitively, the\n",
        "    gradient penalty help stablize the magnitude of the gradients that the\n",
        "    discriminator provides to the generator, and thus help stablize the training\n",
        "    of the generator.\"\"\"\n",
        "    # Get random interpolations between real and fake samples\n",
        "    alpha = torch.rand(real_samples.size(0), 1).cuda()\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples))\n",
        "    interpolates = interpolates.requires_grad_(True)\n",
        "    # Get the discriminator output for the interpolations\n",
        "    d_interpolates = discriminator(interpolates)\n",
        "    # Get gradients w.r.t. the interpolations\n",
        "    fake = torch.ones(real_samples.shape[0], 1).cuda()\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "    # Compute gradient penalty\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x3mgXtVN8ldM"
      },
      "outputs": [],
      "source": [
        "def train_one_step(d_optimizer, g_optimizer, real_samples):\n",
        "    \"\"\"Train the networks for one step.\"\"\"\n",
        "    # Sample from the lantent distribution\n",
        "    latent = torch.randn(batch_size, latent_dim)\n",
        "\n",
        "    # Transfer data to GPU\n",
        "    if torch.cuda.is_available():\n",
        "        real_samples = real_samples.cuda()\n",
        "        latent = latent.cuda()\n",
        "    \n",
        "    # === Train the discriminator ===\n",
        "    # Reset cached gradients to zero\n",
        "    d_optimizer.zero_grad()\n",
        "    # Get discriminator outputs for the real samples\n",
        "    prediction_real = discriminator(real_samples)\n",
        "    # Compute the loss function\n",
        "    d_loss_real = torch.mean(torch.nn.functional.relu(1. - prediction_real))\n",
        "    # Backpropagate the gradients\n",
        "    d_loss_real.backward()\n",
        "    \n",
        "    # Generate fake samples with the generator\n",
        "    fake_samples = generator(latent)\n",
        "    # Get discriminator outputs for the fake samples\n",
        "    prediction_fake_d = discriminator(fake_samples.detach())\n",
        "    # Compute the loss function\n",
        "    d_loss_fake = torch.mean(torch.nn.functional.relu(1. + prediction_fake_d))\n",
        "    # Backpropagate the gradients\n",
        "    d_loss_fake.backward()\n",
        "\n",
        "    # Compute gradient penalty\n",
        "    gradient_penalty = 10.0 * compute_gradient_penalty(\n",
        "        discriminator, real_samples.data, fake_samples.data)\n",
        "    # Backpropagate the gradients\n",
        "    gradient_penalty.backward()\n",
        "\n",
        "    # Update the weights\n",
        "    d_optimizer.step()\n",
        "    \n",
        "    # === Train the generator ===\n",
        "    # Reset cached gradients to zero\n",
        "    g_optimizer.zero_grad()\n",
        "    # Get discriminator outputs for the fake samples\n",
        "    prediction_fake_g = discriminator(fake_samples)\n",
        "    # Compute the loss function\n",
        "    g_loss = -torch.mean(prediction_fake_g)\n",
        "    # Backpropagate the gradients\n",
        "    g_loss.backward()\n",
        "    # Update the weights\n",
        "    g_optimizer.step()\n",
        "\n",
        "    return d_loss_real + d_loss_fake, g_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ukfh1dxIsDw"
      },
      "source": [
        "## Training preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCqTBe3p09xY",
        "outputId": "b9cbbf95-c8df-460c-945c-13031eb0d9ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters in G: 583312\n",
            "Number of parameters in D: 566273\n"
          ]
        }
      ],
      "source": [
        "# Create data loader\n",
        "data_loader = get_data_loader()\n",
        "\n",
        "# Create neural networks\n",
        "discriminator = Discriminator()\n",
        "generator = Generator()\n",
        "n_params_g = sum(p.numel() for p in generator.parameters() if p.requires_grad)\n",
        "n_params_d = sum(p.numel() for p in discriminator.parameters() if p.requires_grad)\n",
        "print(f\"Number of parameters in G: {n_params_g}\")\n",
        "print(f\"Number of parameters in D: {n_params_d}\")\n",
        "\n",
        "# Create optimizers\n",
        "d_optimizer = torch.optim.Adam(\n",
        "    discriminator.parameters(), lr=0.001,  betas=(0.0, 0.9)\n",
        ")\n",
        "g_optimizer = torch.optim.Adam(\n",
        "    generator.parameters(), lr=0.001, betas=(0.0, 0.9)\n",
        ")\n",
        "\n",
        "# Prepare the inputs for the sampler, which wil run during the training\n",
        "sample_latent = torch.randn(n_samples, latent_dim)\n",
        "\n",
        "# Transfer the neural nets and samples to GPU\n",
        "if torch.cuda.is_available():\n",
        "    discriminator = discriminator.cuda()\n",
        "    generator = generator.cuda()\n",
        "    sample_latent = sample_latent.cuda()\n",
        "\n",
        "# Create an empty dictionary to sotre history samples\n",
        "history_samples = {}\n",
        "\n",
        "# Create a LiveLoss logger instance for monitoring\n",
        "liveloss = PlotLosses(outputs=[MatplotlibPlot(cell_size=(6,2))])\n",
        "\n",
        "# Initialize step\n",
        "step = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL499fTNJcSd"
      },
      "source": [
        "## Training iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "JsCO34_A3N2U",
        "outputId": "a8fd7e66-1ae0-4994-85aa-4ed969291ccc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                 | 0/20000 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "No CUDA GPUs are available",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m real_samples \u001b[39m=\u001b[39m images_to_vectors(real_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Train the neural networks\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m d_loss, g_loss \u001b[39m=\u001b[39m train_one_step(d_optimizer, g_optimizer, real_samples)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Record smoothened loss values to LiveLoss logger\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
            "\u001b[1;32m/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb Cell 23\u001b[0m in \u001b[0;36mtrain_one_step\u001b[0;34m(d_optimizer, g_optimizer, real_samples)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m d_loss_fake\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Compute gradient penalty\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m gradient_penalty \u001b[39m=\u001b[39m \u001b[39m10.0\u001b[39m \u001b[39m*\u001b[39m compute_gradient_penalty(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     discriminator, real_samples\u001b[39m.\u001b[39;49mdata, fake_samples\u001b[39m.\u001b[39;49mdata)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Backpropagate the gradients\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m gradient_penalty\u001b[39m.\u001b[39mbackward()\n",
            "\u001b[1;32m/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb Cell 23\u001b[0m in \u001b[0;36mcompute_gradient_penalty\u001b[0;34m(discriminator, real_samples, fake_samples)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\"\"\"Compute the gradient penalty for regularization. Intuitively, the\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mgradient penalty help stablize the magnitude of the gradients that the\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdiscriminator provides to the generator, and thus help stablize the training\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mof the generator.\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Get random interpolations between real and fake samples\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m alpha \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrand(real_samples\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m), \u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m interpolates \u001b[39m=\u001b[39m (alpha \u001b[39m*\u001b[39m real_samples \u001b[39m+\u001b[39m ((\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m alpha) \u001b[39m*\u001b[39m fake_samples))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ivan/projects/mes/vootoobe/mgw/ismir2019tutorial/gan.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m interpolates \u001b[39m=\u001b[39m interpolates\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/projects/mes/vootoobe/mgw/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:217\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[39m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[39m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    218\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    221\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ],
      "source": [
        "# Create a progress bar instance for monitoring\n",
        "if step < n_steps + 1:\n",
        "    progress_bar = tqdm(total=n_steps, initial=step, ncols=80, mininterval=1)\n",
        "else:\n",
        "    print(colored('[Warnings]', 'red'), 'Current step exceeds total step.')\n",
        "\n",
        "# Start iterations\n",
        "while step < n_steps + 1:\n",
        "    # Iterate over the dataset\n",
        "    for real_batch, _ in data_loader:\n",
        "        # Convert input images into vetors\n",
        "        real_samples = images_to_vectors(real_batch)\n",
        "\n",
        "        # Train the neural networks\n",
        "        d_loss, g_loss = train_one_step(d_optimizer, g_optimizer, real_samples)\n",
        "\n",
        "        # Record smoothened loss values to LiveLoss logger\n",
        "        if step > 0:\n",
        "            running_d_loss = 0.05 * d_loss + 0.95 * running_d_loss\n",
        "            running_g_loss = 0.05 * g_loss + 0.95 * running_g_loss\n",
        "        else:\n",
        "            running_d_loss, running_g_loss = 0.0, 0.0\n",
        "        liveloss.update({'d_loss': running_d_loss, 'g_loss': running_g_loss})\n",
        "\n",
        "        # Update losses to progress bar\n",
        "        progress_bar.set_description_str(\n",
        "            f\"(d_loss={d_loss: 8.6f}, g_loss={g_loss: 8.6f})\"\n",
        "        )\n",
        "        \n",
        "        if step % sample_interval == 0:\n",
        "            # Get generated samples\n",
        "            samples = vectors_to_images(generator(sample_latent))\n",
        "            samples = samples.cpu().detach().numpy() \\\n",
        "                             .reshape(n_rows, n_cols, 28, 28) \\\n",
        "                             .transpose((0, 2, 1, 3)) \\\n",
        "                             .reshape(n_rows * 28, n_cols * 28)\n",
        "            history_samples[step] = samples\n",
        "\n",
        "            # Display loss curves\n",
        "            clear_output(True)\n",
        "            if step > 0:\n",
        "                liveloss.send()\n",
        "            \n",
        "            # Display generated samples\n",
        "            plt.figure(figsize=(15, 3))\n",
        "            plt.imshow(samples, cmap='Greys', vmin=0, vmax=1)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            \n",
        "        step += 1\n",
        "        progress_bar.update(1)\n",
        "        if step >= n_steps:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        },
        "id": "XqN1sKoODaCm",
        "outputId": "61d17d81-f6e3-4486-a784-4e2718ca2553"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.6 ('.venv': pipenv)' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/home/ivan/projects/mes/vootoobe/mgw/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Show history\n",
        "steps = [0, sample_interval, 10 * sample_interval, 100 * sample_interval, n_steps]\n",
        "for step in steps:\n",
        "    print(f\"Step={step}\")\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    plt.imshow(history_samples[step], cmap='Greys', vmin=0, vmax=1)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Colab-GAN-demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('.venv': pipenv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "151a08f6f3066d8ea6c8f345eb4c4fdcf54e4fdeef14cfb384ec7210c1f6d726"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
